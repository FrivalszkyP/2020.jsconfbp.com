---
path: "/updates/how-did-we-evaluate-600-talk-proposals"
date: "2020-03-07"
title: "How did we evaluate over 600 talk proposals"
lead: ""
socialCard: "cfp_tool.png"
---
import Button from '../src/components/Button'


Time to time we receive in feedbacks, that the use of Google Forms as a CFP input can be tedious for speakers,
especially those who like to keep a speaker profile, and follow multiple CFP submissions simultaneously.
Reasons behind these concerns are unfortunately true: lots of copy/paste in form fields, if they update the abstract,
they have to manually copy back into the source they're keeping and many smaller issues.

The other question we face sometimes is how could we pick 20 talks from over 600 submissions, which seems like an enormous task to handle.

To reflect on these, we've decided to write this post about the process of how we read and evaluate every CFP we've received,
the tools for this task and the reason behind our chosen tools.

## The most important point

The selection process must be as anonymous and bias-free as possible, until the point where we actually
discussing the final number of talks. The process was developed over the years by the amazing team at JSConf.EU and CSSConf.EU,
its worth reading [their post explaining the details](https://blog.cssconf.eu/2015/08/15/a-talk-selection-process-explained/).

This process **_just works_**, and results in a diverse line-up. If we put enough work in focusing on inclusivity and diversity,
the more people from the most diverse backgrounds would submit a talk, the more diverse the topics, points of views and stories will be.
This process gives space for both black-belt CFP submitters, who have the knowledge and practice to write CFPs that sell easily,
but also gives room for first-time submitters and speakers, who decide to give a talk by themselves, or get inspired at the wonderful
Global Diversity CFP workshops.

We're proud that we had (at least) one first-time international conference speaker each year on our line-up.
Even more proud to see the career path of these people rise, and see them later at conferences all over the world.

## The talk and the curator

You can't read through 600 CFPs and possibly recall that one's was slightly better than the other. So we don't compare them to each other,
that is the last step probably when we see the finalists.

Instead, we aim to evaluate the talk as an attendee, a developer, and a curious mind. Questions we ask ourselves are like:

- Is the topic too advanced or the opposite, too simple?
- In any case of the above, is the talk could be a good learning experience for the most people?
- Could the talk dive deep enough the topic in 30 minutes, or just scratch the surface?
- If it's scratching the surface, is the topic worthwhile, to raise awareness to it, and will the attendees be inspired to dig deeper?

And the two really basic questions that work well with a great curator team with diverse background and knowledge level:

- Would I attend this talk If I've paid for a conference?
- Would I pay for a conference if this talk would be on the schedule?

## Requirements

Based on experiences over the past years, we've collected and summarized the requirements for the tool that fully supports our CFP process.

- we need complete control over what fields we put in the CFP form
- completely anonymized through the whole process until the very end
- can't see other team member votes, to avoid further bias
- everybody should be able to vote on every submission
 Can't make an honest decision if someone omits even 1% of the talks.
- it should handle privacy, adhere GDPR
 The more data handlers we include, the more convoluted is our data privacy policy. The safest data stored is data without personal details.
- it's easy to access
 Every time from everywhere, on every device.
- easy to read
- easy to score

## The Curator Experience

As members of a team to organize and curate a non-profit conference, we're all busy otherwise, so we have to do this in our spare time.
It can happen during the daily commute, standing in line while shopping for groceries, or having a dedicated time of the day for just this - we're all different.
Either way, the tool that makes the evaluation possible has to be the smallest obstacle, so we don't get discouraged of reading and scoring,
or annoyed by the tool itself that would affect our judgment.

At a very low level, the selection work itself is a UX issue:

- mobile-friendly so we can read submissions at any time and place on our mobiles
- clean, no cluttering, no distractions, just the submission and you
- simple, read and score
- keep track of your progress, pause and continue any time and as many times as possible


## The CFP tool landscape

Many of us have heard that _"existing tools should not be re-invented"_, so this might also be true for CFP evaluation tools. We've looked around several times,
and keeping an eye on the development of CFP some tool services. Let's summarize what we've seen so far:

Google Forms
- free form builder can have as many fields in as many ways we want
- not anonym unless we copy field columns by hand or by some script
- Google handles PI data
- relatively easy to access
- terrible to read and use as an evaluation tool, even worse on mobile
- score field can be disambiguous, hard to evaluate later
- has an API via Google Sheets API

Sessionize

- almost free form editor, certain fields cannot be ordered or removed
- has an anonymous mode for users almost perfect, except in some points
 - submission dates are still visibly tied to submission titles
 this could be problematic if someone emails us "I've submitted something on this day ..."
 - we can see if someone submits multiple talks, even without names
 - can't control what fields are considered visible in anonymous mode in the form editor
- Offers a solution for evaluating, but it goes in the opposite direction from ours
 - "The simplest way to evaluate submitted sessions by a content team is to compare them side by side." that's probably the last step we do
- has an API but it seems it's for the final schedule
- does not care if an event has a CoC or not :/


It seemed like none of these were perfect for our requirements, so after struggling with CFP evaluation for some years, we've rolled out our own.

## Our CFP evaluation app

[Here it is](https://cfp-vote.herokuapp.com/), you can try it out with a GitHub user.
The code for the app is on [GitHub](https://github.com/JSConfBp/cfp-vote) you can instantly deploy to your event using Heroku, if you wish to try it out.

To collect submissions we needed some kind of a form system that is maintained, usable, meet our requirements regarding forms, and has an API for the data.
This is why we're currently using Google Forms, it has everything for this task.

Our app we finally developed handles talk evaluating and scoring, and keeps track of our progress. It fetches CFP submissions from Google Sheets,
but to keep them anonymized, it fetches only selected fields. This is also GDPR compliant, as it does not transfer personal information.
Each talk gets an assigned random ID and the users of the app can vote once on each submission. No skipping talks and no going back and updating votes.
After a vote was cast, the app shows the next submission, until we vote on each and every one of them.

We can see the progress of the team members, and the histogram of votes for talks.
