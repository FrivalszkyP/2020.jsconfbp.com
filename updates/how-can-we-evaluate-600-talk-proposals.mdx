---
path: "/updates/how-can-we-evaluate-600-talk-proposals"
date: "2020-03-07"
title: "How can we evaluate over 600 talk proposals"
lead: "Reasons on how and why we evaluate CFPs in the way we do, and the about tools that we use to help us."
socialCard: "cfp_tool.png"
---
import Button from '../src/components/Button'
import PostImage from '../src/components/PostImage'

Time to time we receive in feedbacks, that the use of Google Forms as a CFP input can be tedious for speakers,
especially those who like to keep a speaker profile, and follow multiple CFP submissions simultaneously.
Reasons behind these concerns are unfortunately true: lots of copy/paste in form fields, hard to keep track of quick, spontaneous updates of the abstract
and many smaller issues.

The other question we face sometimes is how could we pick 20 talks from over 600 submissions, which seems like an enormous task to handle.

To reflect on these, we've decided to write this post about the process of how we read and evaluate every CFP we've received,
the tools for this task and the reason behind our chosen tools.

## The most important point

**The selection process must be as anonymous and bias-free as possible,** until the point where we actually
discussing the final number of talks. The process was developed over the years by the amazing team at JSConf.EU and CSSConf.EU,
its worth reading [their post explaining the details](https://blog.cssconf.eu/2015/08/15/a-talk-selection-process-explained/).
Julia Evans from !!Con [wrote a great article](https://jvns.ca/blog/2014/06/06/should-my-conference-do-anonymous-review/) on the benefits of these processes

This process **_just works_** and results in a wonderful and diverse line-up. If we put enough work in focusing on inclusivity and diversity,
the more people from the most diverse backgrounds would submit a talk, the more diverse the topics, points of views and stories will be.
This process gives space for both black-belt CFP submitters, who have the knowledge and practice to write CFPs that sell easily,
but also gives room for first-time submitters and speakers, who decide to give a talk by themselves, or get inspired at the [wonderful
Global Diversity CFP workshops](https://www.globaldiversitycfpday.com/).

We're proud that we had (at least) one first-time international conference speaker each year on our line-up.
Even more proud to see the career path of these people rise, and see them later at conferences all over the world.

## The talk and the curator

You can't read through 600 CFPs and possibly recall that one's was slightly better than the other. So we don't compare them to each other,
that is the last step probably when we see the finalists.

Instead, we aim to evaluate the talk as an attendee, a developer, and a curious mind. Questions we ask ourselves are like:

- Is the topic too advanced - or the opposite, too simple?
- In any case of the above, is the talk could be a good learning experience for most people?
- Could the talk dive deep enough the topic in 30 minutes, or just scratch the surface?
- If it's scratching the surface, is the topic worthwhile, to raise awareness to it, and will the attendees be inspired to dig deeper?

And the two really basic questions that work well with a great curator team with diverse background and knowledge level:

- Would I attend this talk If I've paid for a conference?
- Would I pay for a conference if this talk would be on the schedule?

## Requirements

To help us evaluate in a way we see optimal, we need the best tool that supports us. Based on experiences over the past years,
we've collected and summarized the requirements for the tool that fully supports our CFP process.

- completely anonymized through the whole process until the very end
- we need total control over what fields we put in the CFP form
- can't see other team member votes, to avoid further bias
- everybody should be able to vote on every submission. Can't make an honest decision if someone omits even 1% of the talks.
- it should handle privacy, adhere to GDPR. The more data handlers we include, the more convoluted is our data privacy policy.
The safest data stored and moved is data without personal details.
- it's easy to access. Every time from everywhere, on every device.
- easy to read
- easy to score

## The Curator Experience

As members of a team who organize and curate a non-profit conference, we're all busy otherwise, so we have to do this in our spare time.
It can happen during the daily commute, standing in line while shopping for groceries, or having a dedicated time of the day for just this - we're all different.
Either way, the tool that makes the evaluation possible has to be the smallest obstacle, so we don't get discouraged of reading and scoring,
or annoyed by the tool itself that would affect our judgment.

At a very low level, the selection work itself is a UX issue:

- mobile-friendly so we can read submissions at any time and place on our mobiles
- clean, no cluttering, no distractions, just the submission, and the curator
- simple to read and to give a score
- keep track of our progress, pause and continue any time, so we don't feel pressured and frustrated


## The CFP tool landscape

Many of us have heard that _"existing tools should not be re-invented"_, so this might also be true for CFP evaluation tools. We've looked around several times,
and keeping an eye on the development of some CFP services. Let's summarize what we've seen so far:

**Google Forms**

- it's a free form builder, can have as many fields in as many ways we want
- not anonym, unless we copy field columns by hand or by some script
- Google handles personal data
- relatively easy to access
- terrible to read and use as an evaluation tool, even worse on mobile
- score field can be disambiguous, hard to evaluate later
- has an API via Google Sheets API

**Sessionize**

Their developers have put a lot of effort into [Sessionize](https://sessionize.com/), which grew a lot in the past years, and really looks great and promising.

- almost free form editor, certain fields cannot be ordered or removed
- has an anonymous mode for users almost perfect, except in some points
 - submission dates are still visibly tied to submission titles. This could be problematic if someone emails us, saying "I've submitted something on this day ..."
 - we can see if someone submits multiple talks, even without names
 - can't control what fields are considered visible in the anonymous mode in the form editor
- Offers a solution for evaluating, but it goes in the opposite direction from ours
 - "The simplest way to evaluate submitted sessions by a content team is to compare them side by side." that's probably the last step we do
- has an API but it seems it's only for the final schedule
- does not care if an event has a CoC or not, which is kind of discouraging


**PaperCall**

[PaperCall.io](https://www.papercall.io/) was maybe the first CFP/Event handling service I've encountered years ago, provides a simpler solution that Sessionize.

- no or limited control over the fields
- has a field for Code of Conduct, pre-populated from http://confcodeofconduct.com
- support anonymized rating and processing of submissions
- can pick a rating style between stars and some phrases that are tied to scores
- has webhooks and an API to access submitted CFPs, but only in the paid Professional package and could not find any documentation on it without purchasing a license

It seemed like none of these were perfect for our requirements, so after struggling with CFP evaluation for some years, we've rolled out our own.

## Our CFP evaluation app

**[Here it is, you can try it out with a GitHub user](https://cfp-vote.herokuapp.com/)**, the demo version has 200 randomly generated CFPs uploaded.

<a href="https://cfp-vote.herokuapp.com/"><PostImage src="app-example" alt="Some screens saved from our demo CFP app" /></a>

The code for the app is on [GitHub](https://github.com/JSConfBp/cfp-vote), you can instantly deploy to your event using Heroku if you wish to try it out with your setup.

To collect submissions we needed some kind of a form system that is maintained, usable, meet our requirements regarding forms, and has an API for the data.
This is why we're currently using Google Forms, it has everything for this task. If Sessionize could have an API for fetching submitted talks from them,
that would be awesome, we could use that until then we probably stay with Google Forms.

Our team enjoyed the talk evaluation, they said it was nice and effective.

- The app handles talk evaluating and scoring and keep track of our progress.
- It fetches CFP submissions from Google Sheets, but to keep them anonymized, it fetches only selected fields.
- This is also GDPR compliant, as it does not transfer personal information to Heroku
- The users of the app can vote once on each submission. No skipping talks and no going back and updating votes.
- After a vote was cast, the app shows the next submission, until we vote on each and every one of them.
- We can put our mobiles away any time, and the next time we return to the app, continue from where we left.

As for the team, we can see the progress of the team members, in percentages, and the histogram of votes for talks.
Team members can be added from GitHub - or can be opened up for any GitHub user, like in our demo.

We even thinking about making the progress percentage public, so we can use that information on our site or on social media.

If you're curious, [try it out](https://cfp-vote.herokuapp.com/). We welcome any feedback, questions, and suggestions. If you run an event
and decide to handle your CFP the same way, we're happy to help you!

We hope this way more conferences can have a diverse lineup, like the ones in the JSConf Family ðŸ’–
